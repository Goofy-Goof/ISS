\documentclass[12pt]{extarticle}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}
\usepackage[
    backend=biber,
    bibstyle=ieee,
    citestyle=numeric,
]{biblatex}
\addbibresource{cite.bib} %Imports bibliography file
\usepackage{soul}
\usepackage[export]{adjustbox}
\usepackage{listings}
\usepackage{makecell}
\usepackage{longtable}
\usepackage[table,x11names]{xcolor}
% Set page size and margins
% Replace `letterpaper' with`a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}
\usepackage{textcomp}

\title{
    Pretext Tasks: \\
    can they help against adversarial attacks?
}
\author{Artem Sereda artem.sereda@campus.tu-berlin.de}

\begin{document}
    \maketitle

    \begin{abstract}
    The continuous improvements in the image classification over the recent years are opening doors to
    a lot of life-changing applications of Machine Learning (ML).
    It has been proven that including a pretext task improves classifiers' standard accuracy.
    Despite numerous studies of an adversarial vulnerability phenomenon, there is no clear answer
    how a choice of pre-training influences Neural Networks' (NNs') adversarial robustness.
    This paper will first give a general introduction to white box adversarial attacks using Fast Gradient Sign Method (FGSM),
    as well as to some popular pretext tasks.
    Afterwards the impact of including a pretext task, as well as its choice will be evaluated.
    The implementation of the evaluation for this paper can be found in accompanying GitHub repository~\cite{github}.
    \end{abstract}


    \include{intro}
    \include{methods}
    \include{results}
    \include{discussion}


    \printbibliography
\end{document}
