@article{DBLP:journals/corr/abs-1802-08195,
  author    = {Gamaleldin F. Elsayed and
               Shreya Shankar and
               Brian Cheung and
               Nicolas Papernot and
               Alex Kurakin and
               Ian J. Goodfellow and
               Jascha Sohl{-}Dickstein},
  title     = {Adversarial Examples that Fool both Human and Computer Vision},
  journal   = {CoRR},
  volume    = {abs/1802.08195},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.08195},
  eprinttype = {arXiv},
  eprint    = {1802.08195},
  timestamp = {Mon, 13 Aug 2018 16:46:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1802-08195.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1912-01991,
  author    = {Ishan Misra and
               Laurens van der Maaten},
  title     = {Self-Supervised Learning of Pretext-Invariant Representations},
  journal   = {CoRR},
  volume    = {abs/1912.01991},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.01991},
  eprinttype = {arXiv},
  eprint    = {1912.01991},
  timestamp = {Thu, 02 Jan 2020 18:08:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-01991.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1905-11946,
  author    = {Mingxing Tan and
               Quoc V. Le},
  title     = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1905.11946},
  year      = {2019},
  url       = {http://arxiv.org/abs/1905.11946},
  eprinttype = {arXiv},
  eprint    = {1905.11946},
  timestamp = {Mon, 03 Jun 2019 13:42:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1905-11946.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{goodfellow2015explaining,
      title={Explaining and Harnessing Adversarial Examples},
      author={Ian J. Goodfellow and Jonathon Shlens and Christian Szegedy},
      year={2015},
      eprint={1412.6572},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{DBLP:journals/corr/NorooziF16,
  author    = {Mehdi Noroozi and
               Paolo Favaro},
  title     = {Unsupervised Learning of Visual Representations by Solving Jigsaw
               Puzzles},
  journal   = {CoRR},
  volume    = {abs/1603.09246},
  year      = {2016},
  url       = {http://arxiv.org/abs/1603.09246},
  eprinttype = {arXiv},
  eprint    = {1603.09246},
  timestamp = {Mon, 13 Aug 2018 16:49:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/NorooziF16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{kolesnikov2019revisiting,
      title={Revisiting Self-Supervised Visual Representation Learning},
      author={Alexander Kolesnikov and Xiaohua Zhai and Lucas Beyer},
      year={2019},
      eprint={1901.09005},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{russakovsky2015imagenet,
      title={ImageNet Large Scale Visual Recognition Challenge},
      author={
      Olga Russakovsky and Jia Deng and Hao Su and
      Jonathan Krause and Sanjeev Satheesh and Sean Ma and
      Zhiheng Huang and Andrej Karpathy and Aditya Khosla and
      Michael Bernstein and Alexander C. Berg and Li Fei-Fei
      },
      year={2015},
      eprint={1409.0575},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{ilyas2019adversarial,
      title={Adversarial Examples Are Not Bugs, They Are Features},
      author={Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras and Logan Engstrom and Brandon Tran and Aleksander Madry},
      year={2019},
      eprint={1905.02175},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@InProceedings{pmlr-v97-locatello19a,
  title = 	 {Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations},
  author =       {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {4114--4124},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/locatello19a/locatello19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/locatello19a.html},
  abstract = 	 {The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than $12000$ models covering most prominent methods and results metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties “encouraged” by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets.}
}
