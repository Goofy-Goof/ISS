\section{Discussion}

\paragraph{Goal}In this paper, I set up a goal to investigate if including a pretext task in the training process could reduce NNs
vulnerability against adversarial attacks.
The phenomenon of adversarial examples was extensively studied~\cite{ilyas2019adversarial, DBLP:journals/corr/abs-1802-08195, goodfellow2015explaining},
as well as its relation with standard accuracy~\cite{https://doi.org/10.48550/arxiv.1805.12152}.
However, ss to the day, there have been no studies of how choice of pretext task influences adversarial vulnerability.
There are also numerous studies about the influence of pretext tasks on standard
accuracy~\cite{DBLP:journals/corr/abs-1912-01991, DBLP:journals/corr/NorooziF16, kolesnikov2019revisiting}.


\paragraph{Observations and Interpretation}
All 3 pre-training approaches have shown a slight improvement.
Accuracy was increased by all pre-text tasks, with jigsaw performing the best and being up to 8\% more accurate.
Transfer learning and no pre-training had shown no decrease in miss-classification rate.
Rotation and jigsaw were able to reduce miss-classification rate up to 5\%, starting up from 45 training epochs.
\st{pretext task enforce learning of important features ...}
\\
Despite keeping the intensity of adversarial pattern fixed at a very low value($\epsilon = 0.01$),
FGSM was still able to produce at least 95\% of miss-classifications.
The explanation for this liest in fact, that FGSM evaluates loss function of NN,
so it would be natural to assume that high standard classification accuracy pairs with high adversarial vulnerability.
\\
The improvements seen in the results section, could also be caused by other factors, not only choice of pre-text task.
For example, tf flowers consist of 3670 items.
During pretext training, 4 pseudo classes were generated,
which results in 14680 images, and \textbf{18350} images together with downstream training.
Imagenette consists of 13,394, which results in \textbf{17064} images together with downstream training.
So the better results for jigsaw and rotation, compared with transfer learning,
could also be caused by bigger training dataset size.

\paragraph{Limitations}This study naturally comes with a few limitations.
For an instance, the fact that the evaluation was done only for one (relatively small) dataset,
therefore, the result could be bound to the biases existing in it.
In addition, that hyperparameters of NN (learning-rate, loss function etc.)
were not fine-tuned, which leaves the question open whether the
results presented have a causal relation with the choice of pretext task, or is it just correlation,
and similar results could be achieved by better chosen hyperparameters for NN .

\paragraph{Future work}
Despite all the recent advances in visual representation learning, and the success of ML in general, the phenomenon of
adversarial vulnerability still remains unsolved.
ML provides a lot of fascinating possibilities, hence its reliability and robustness are still an open question.
Researchers should explore new methods and approaches to ensure them.
Concrete for pretext-tasks, this research could be extended by doing similar evaluation on different datasets,
fine-tuning NNs hyperparameters,
investing more time in pre-training as well as using models with a bigger number of trainable parameters
a.e. \ EfficientNetB1, EfficientNetB2 \ldots EfficientNetB7.

\paragraph{Conclusion}
Including rotation or jigsaw pretext task in a training process had shown slight improvements in adversarial robustness.
However, it is not clear whether these improvements were caused by the choice of pretext task, or other factors.
In conclusion, adversarial vulnerability remains an open topic,
and researchers are still far from being able to solve it.
This investigation was not able to show any substantial improvements in adversarial robustness.
\\
Nevertheless, rotation and jigsaw had once again been proved useful in increasing standard classification accuracy.
Therefore, it would be safe to say,
that including either of those in the training process could be considered a good practise.


