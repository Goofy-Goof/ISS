\section{Discussion}

\paragraph{Goal}In this paper,
the goal of investigating how a choice of pretext task can influence NNs adversarial vulnerability was set up.
To address this question, the average of miss rate over 20 evaluation rounds for different pretext tasks was compared
(while to keeping $\epsilon$ fixed at 0.01).
The miss rate shows how likely is it for fine-tuned NN to be fooled by an FGSM adversarial attack,
with $\epsilon = 0.01$.
Naturally the lower values of miss-rate, is what shpuld be aimed for.
Parallel to this accuracy in each evaluation was recorded, as only robust classifiers, which retain high standard
classification accuracy make sense.
Recorded accuracy simply represents number of images, which were correctly classified before applying FGSM.


\paragraph{Observations and Interpretation}
Pre-training NN on larger dataset, and the fine-tuning (transfer learning) was able to increase accuracy by up to 6\%.
However, no improvement was seen in the miss-classification rate.
In contrast, rotation and jigsaw were able to show up to 8\% accuracy improvements,
and up to 5\% decrease in miss-classification rate, starting up from 45 training epochs.
\\
Despite keeping the intensity of adversarial pattern fixed at a very low value($\epsilon = 0.01$),
FGSM was still able to produce at least 95\% of miss-classifications.
The explanation for this liest in fact, that FGSM evaluates loss function of NN,
so it would be natural to assume that high standard classification accuracy pairs with high adversarial vulnerability.
\\
Nonetheless, rotation and jigsaw while providing even better improvements in accuracy than transfer learning,
were able to slightly reduce miss-classification rate.
This concurs the theory,
that self-supervised leaning framework improves the quality of representations
learned by NN~\cite{kolesnikov2019revisiting}.



\paragraph{Limitations \&\& Future work}
This study naturally comes with a few limitations.
For an instance, the fact that the evaluation was done only for one (relatively small) dataset,
therefore, the result could be bound to the biases existing in it.
In addition, that hyperparameters of NN (learning-rate, loss function etc.)
were not fine-tuned, which leaves the question open whether the
results presented have a causal relation with the choice of pretext task,
or similar results could be achieved by better chosen hyperparameters of NN.

Improvements seen in the results section, could also be caused by other factors.
For example, the amount of training data.
TF flowers consist of 3670 items.
During pretext training, 4 pseudo classes were generated,
which results in 14680 images, and \textbf{18350} images together with downstream training.
Imagenette consists of 13,394, which results in \textbf{17064} images together with downstream training.

\paragraph{Future work}
Despite all the recent advances in visual representation learning, and the success of ML in general, the phenomenon of
adversarial vulnerability still remains unsolved.
ML provides a lot of fascinating possibilities, hence its reliability and robustness are still an open question.
Researchers should explore new methods and approaches to ensure them.
Concrete for pretext-tasks, this research could be extended by doing similar evaluation on different datasets,
fine-tuning NNs hyperparameters,
investing more time in pre-training as well as using models with a bigger number of trainable parameters
a.e. \ EfficientNetB1, EfficientNetB2 \ldots EfficientNetB7.

\paragraph{Conclusion}
Including rotation or jigsaw pretext task in a training process had shown \textbf{very} slight improvements in adversarial robustness.
However, it is not clear whether these improvements were caused by the choice of pretext task, or other factors.
Unfortunatelly the quality and amount of data is not sufficient to draw a concrete conlusion from it.
Extending this research, by using different NNs with better fine-tuned hyperparameters, evaluation similar metrics on
multiple different datasets and other pretext tasks could provide a solid foundaton for a knowledge how a pretext task choice
infuences adversarial vulnerability.


In conclusion, adversarial vulnerability remains an open topic,
and researchers are still far from being able to solve it.
This investigation was not able to show any substantial improvements in adversarial robustness.
\\
Nevertheless, rotation and jigsaw had once again been proved useful in increasing standard classification accuracy.
Therefore, it would be safe to say,
that including either of those in the training process could be considered a good practise.


