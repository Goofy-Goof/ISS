\section{Discussion}

\paragraph{Goal}In this paper, I set up a goal to investigate if including a pretext task in the training process could reduce NNs
vulnerability against adversarial attacks.
The phenomenon of adversarial examples was extensively studied~\cite{ilyas2019adversarial, DBLP:journals/corr/abs-1802-08195, goodfellow2015explaining},
as well as its relation with standard accuracy~\cite{https://doi.org/10.48550/arxiv.1805.12152}.
However, ss to the day, there have been no studies of how choice of pretext task influences adversarial vulnerability.
There are also numerous studies about the influence of pretext tasks on standard
accuracy~\cite{DBLP:journals/corr/abs-1912-01991, DBLP:journals/corr/NorooziF16, kolesnikov2019revisiting}.


\paragraph{Observations and Interpretation}
All 3 pre-training approaches have shown a slight improvement.
Accuracy was increased by all pre-text tasks, with jigsaw performing the best and being up to 8\% more accurate.
Transfer learning and no pre-training had shown no decrease in miss-classification rate.
Rotation and jigsaw were able to reduce miss-classification rate up to 5\%, starting up from 45 training epochs.
\\
FGSM evaluates loss function of NN,
so it would be natural to assume that higher accuracy pairs with higher adversarial vulnerability.
\\
TF Flowers consist of 3670 items.
During pretext training, 4 pseudo classes were generated,
which results in 14680 images, and \textbf{18350} images together with downstream training.
Imagenette consists of 13,394, which results in \textbf{17064} images together with downstream training.
So the improvements could partially be caused by bigger training dataset size.
\\
\ldots
\\
The experiment was done for a $\epsilon = 0.01$ (which are not even visible for human viewer).
And the improvement of 5\% is far from solving the problem of adversarial vulnerability.

\paragraph{Limitations}This study comes with a few limitations,
most of them coming from to time and resource limitation, as well as working standalone on the research question.
\\
The most important one is the fact that the evaluation was done only for one (relatively small) dataset,
therefore, the result could be bound to the biases existing in it.
\\
Another one is the fact, that hyperparameters of NN (learning-rate, loss function etc.)
were not fine-tuned, which leaves the question open whether the
results presented have a causal relation with the choice of pretext task, or is it just correlation,
and similar results could be achieved by better chosen hyperparameters for NN .

\paragraph{Future work}
Adversarial vulnerability has been around \st{always?}, coming in pair with all the successes in image classification.
As ML provides a lot of fascinating possibilities, its reliability and robustness are still an open question.
Researchers should explore new methods and approaches to ensure them.
Concrete for pretext-tasks, this research could be extended by evaluating it on different datasets, fine-tuning NNs
hyperparameters, investing more time in pre-training as well as using models with a bigger number of trainable parameters
a.e. \ EfficientNetB1, EfficientNetB2 \ldots EfficientNetB7.

\paragraph{Conclusion}
Pretext training was able to produce a little improvement in accuracy as well as robustness against adversarial attacks.
However, these successes are far solving the problem of NNs vulnerability to adversarial attacks.

