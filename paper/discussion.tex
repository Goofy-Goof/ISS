\section{Discussion}

\paragraph{Goal}In this paper, I set up a goal to investigate if including a pretext task in the training process could reduce NNs
vulnerability against adversarial attacks.
The phenomenon of adversarial examples was extensively studied~\cite{ilyas2019adversarial, DBLP:journals/corr/abs-1802-08195, goodfellow2015explaining},
as well as its relation with standard accuracy~\cite{https://doi.org/10.48550/arxiv.1805.12152}.
However, ss to the day, there have been no studies of how choice of pretext task influences adversarial vulnerability.
There are also numerous studies about the influence of pretext tasks on standard
accuracy~\cite{DBLP:journals/corr/abs-1912-01991, DBLP:journals/corr/NorooziF16, kolesnikov2019revisiting}.


\paragraph{Observations and Interpretation}
All 3 pre-training approaches have shown an improvement in accuracy from 5\% to 8\%.
Rotation and jigsaw were able to reduce miss-classification up to 5\% in comparison with transfer learning and no pre-training.
\\
\ldots
\\
The experiment was done for a $\epsilon = 0.01$ (which is not even visible for human viewer).
And the improvement of 5\% is far from solving the problem of adversarial vulnerability.

\paragraph{Limitations}This study comes with a few limitations,
most of them coming from to time and resource limitation, as well as working standalone on the research question.
The most important one is the fact that the evaluation was done only for one (relatively small) dataset,
therefore, the result could be bound to the biases existing in it.
Another one is the fact, that hyperparameters of NN (learning-rate, loss function etc.)
were not fine-tuned, which leaves the question open whether the
results presented have a causal relation with the choice of pretext task, or is it just correlation,
and similar results could be achieved by better chosen hyperparameters for NN .

\paragraph{Future work}
Adversarial vulnerability has been around \st{always?}, coming in pair with all the successes in image classification.
As ML provides a lot of fascinating possibilities, its reliability and robustness are still an open question.
Researchers should explore new methods and approaches to ensure them.
Concrete for pretext-tasks, this research could be extended by evaluating it on different datasets, fine-tuning NNs
hyperparameters, investing more time in pre-training as well as using models with a bigger number of trainable parameters
a.e. \ EfficientNetB1, EfficientNetB2 \ldots EfficientNetB7.

\paragraph{Conclusion}
Pretext training was able to produce a little improvement in accuracy as well as robustness against adversarial attacks.
However, these successes are far solving the problem of NNs vulnerability to adversarial attacks.

