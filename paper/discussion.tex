\section{Discussion}

\paragraph{Goal}
In this paper I set up a goal to investigate if including pretext task in training process could reduce NNs
vulnerability against adversarial attacks.
In contrast, to previous work~\cite{https://doi.org/10.48550/arxiv.1805.12152} the goal was to investigate, weather
robustness could be improved without a tradeoff in standard classification accuracy.

\paragraph{Observations and Interpretation}
All 3 pre-training approaches have shown an improvement in accuracy from 5\% to 8\%.
Rotation and jigsaw were able to reduce miss-classification up to 5\% in comparison with transfer learning and no pre-training.
FGSM uses the gradient of loss function computed with respect to NNs weight.
Better accuracy would mean higher penalties for miss-classification, which result in more sharp gradients, which
contribute more to perturbations generated by FGSM.
At first, it looks like the result presented in previous section contradict with this statement, however
the experiment was done for a $\epsilon = 0.01$ (which is not even visible for human viewer).
And the improvement of 5\% is far from solving the problem of adversarial vulnerability.

\paragraph{Limitations}
This study comes with a few limitation,
most of them due to time and resource limitation, as well as working standalone on the research question.
The evaluation was done only for one (relatively small) dataset, therefore the result could be bound to the biases existing in it.
Hyperparameters of NN (learning-rate, loss function etc.) were not fine-tuned, which leaves the question open whether the
results presented have causal relation with choice of pretext task, or is it just correlation, and similar results could
be achieved by better chosen hyperparameters for NN.

\paragraph{Future work}
Adversarial vulnerability has been around \st{always?}, coming in pair with all the successes in image classification.
As ML provides a lot of fascinating possibilities, it's reliability and robustness is still an open question.
The researches should explore new methods and approaches to ensure them.
Concrete for pretext-tasks, this research could be extended by evaluating it on different datasets, fine-tuning NNs
hyperparameters, investing more time in pre-training as well as using models with bigger number of trainable parameters
a.e. \ EfficientNetB1, EfficientNetB2 \ldots EfficientNetB7.

\paragraph{Conclusion}
Pretext training was able top produce a little improvement in accuracy as well as robustness against adversarial attacks.
However, these successes are far solving the problem of NNs vulnerability to adversarial attacks.

