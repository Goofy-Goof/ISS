\section{Introduction}


\paragraph{Motivation}
It’s widely known fact that NN are vulnerable against adversarial images.
~\cite{ilyas2019adversarial} "Particularly worrisome is the phenomenon of adversarial examples,
imperceptibly perturbed natural inputs that induce erroneous predictions in state-of-the-art classifiers."


\paragraph{Research question}
In this research I would like to evaluate how including state-of-the-art pretext in training process influence N
Ns vulnerability against adversarial attacks.




\subsection{Background and Related work}

\paragraph{Adversarial attack}
Goodfellow defined adversarial attacks as “inputs to machine learning models that an
attacker has intentionally designed to cause the model to make a mistake.” ~\cite{DBLP:journals/corr/abs-1802-08195} \\
In the domain of image classification, adversarial attacks are images usually formed by applying a small perturbation
(which is barely noticeable for human viewer) to a naturally occurring image, with intention to make NN miss-classify.

\paragraph{Fast gradient sign method}
The name was first introduced by Goodfellow and Jonathon Shlens and Christian Szegedy
~\cite{goodfellow2015explaining} as guaranteed approach to make artificial NN miss-classify an image,
which still would de be recognisable as of the same class for human viewer.
Fast gradient sign method (further denoted as FGSM) works by using the gradients of the neural network to create an adversarial pattern.
For an input image, the method evaluates the signed gradient of the loss function with respect to the input image to create a pattern,
which maximises the loss.
The pattern is then added pixel wise to original image.
The new image is called the adversarial image.
The process can be summarised using the following expression:
\begin{equation}
    adv\_x = x + \epsilon \cdot sign(\nabla_x J(\theta, x, y))
\end{equation}
(where $\epsilon$ denotes the intensity of adversarial pattern).

\paragraph{Pretext task}
Pretext task is the self-supervised learning task solved to learn visual representations,
with the aim of using the learned representations or model weights obtained in the process, for the downstream task.
It has been showm, that pretext tasks can significantly improve NNs accuracy.~\cite{kolesnikov2019revisiting}
It is also believed they contribute to NNs learning of important (as per human agents) features.

\paragraph{Efficient Net}
Convolutional networks' architectures for image recognition have evolved quite drastically in recent years, with numerous options available "out of the box".
A.e.Efficient Net (further denoted as EffNet) delivers impressive accuracy, while being able to scale better than a lot of previous architectures~\cite{DBLP:journals/corr/abs-1905-11946}.
All the evaluations were \st{done} using it, namely EfficientNetB0.

\paragraph{Rotation pretext task}
A common choice of pretext task could be to produce 4 copies of
a single image by rotating it by {0°, 90°, 180°, 270°} and let a single network predict the rotation which was applied.
~\cite{kolesnikov2019revisiting}
Intuitively, a good model should learn to recognize canonical orientations of objects in natural images."

\paragraph{Jigsaw pretext task}
The task is to recover relative spatial position of 4 sampled image patches after a random permutation of these patches was performed.
~\cite{kolesnikov2019revisiting}
All of these patches are concatenated in 'puzzle' image, which is later sent through same network, which needs to predict a permutation applied.
In practice, 4 out of 24 possible permutations were used.




\section{Methods}


\paragraph{Rotation pretext task}
Each image from original dataset was rotated 0°,90°,180°,270° and assigned new pseudo label:
\begin{itemize}
    \item 0° \textrightarrow 0
    \item 90° \textrightarrow 1
    \item 180° \textrightarrow 2
    \item 270° \textrightarrow 3
\end{itemize}
all 4 batches of rotated images were concatenated in dataset, and shuffled.
Then last dense layer of NN was replaced with dense layer for corresponding number of pseudo classes (4).
Then same model was then trained to identify rotation applied.
\\
\begin{figure}[!htp]
    \begin{subfigure}{0.33\textwidth}
        \caption{Label = 0}
        \includegraphics[width=5cm]{images/rot_0}
    \end{subfigure}
    \begin{subfigure}{0.2\textwidth}
        \caption{Label = 1}
        \includegraphics[width=5cm]{images/rot_1}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \caption{Label = 2}
        \includegraphics[width=5cm]{images/rot_2}
    \end{subfigure}
\end{figure}

\paragraph{Jigsaw pretext task}
For jigsaw I have adopted similar approach as described by Mehdi Noroozi and Paolo Favaro~\cite{DBLP:journals/corr/NorooziF16}.
Image was cut in 4 equal parts, 4 out of 24 possible premutations were chosen for each batch.
(number of possible permutation can be obtained from Newtonian binomial $P=\frac{r!}{(r-n)!}$).
Similarly to rotation pseudo labels in [0\ldots23] have been assigned, data was shuffled and dense layer was replaced by suitable one.
The network then is trained to identify permutation applied.
\\
\begin{figure}
    \begin{subfigure}{0.33\textwidth}
        \caption{Original image}
        \includegraphics[width=5cm]{images/dandelion}
    \end{subfigure}
    \begin{subfigure}{0.2\textwidth}
        \includegraphics[width=3cm]{images/arrow}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \caption{Generated puzzle, with label=1}
        \includegraphics[width=5cm]{images/puzzle}
    \end{subfigure}
\end{figure}

\paragraph{Adversarial Images with FGSM}
My implementation of FGSM is based on \href{https://www.tensorflow.org/tutorials/generative/adversarial_fgsm}{TF FGSM}.
In order to generate adversarial pattern for each image, gradient of loss function is evaluated with sign for each image.
Then it's overlapped with original image with $\epsilon$ in [0.01, 0.1, 0.15], as any higher values make the image visually disrupted
for human viewer(like me).
\\
\begin{figure}[!h]
    \begin{subfigure}{0.33\textwidth}
        \caption{Labrador retriever 41.82\% confidence}
        \includegraphics[width=5cm]{images/labrador}
    \end{subfigure}
    \begin{subfigure}{0.33\textwidth}
        \caption{Adversarial pattern}
        \includegraphics[width=5cm]{images/adv_pattern}
    \end{subfigure}
    \begin{subfigure}{0.3\textwidth}
        \caption{Weimaraner 15.13\% confidence}
        \includegraphics[width=5cm]{images/adv_labrador}
    \end{subfigure}
\end{figure}

\paragraph{Evaluation approach}
In order to evaluate, how including pretext task in training process influences NNs vulnerability against adversarial attacks,
I have measured intensity of adversarial pattern needed to make NN miss classify (Further denoted with epsilon or $\epsilon$).
\newline
5\% of dataset were reserved for experiment.
During each evaluation round NN network was trained with either none, rotation, jigsaw, or both, pretext tasks.
Number of training epochs was varied in [10, 20, 30, 50] for pretext and downstream task.
\\
Reserved images were \st{fed} to NN, the ones which were correctly classified saved.
Then each of saved (previously correctly classified images) was overlapped with adversarial pattern generated for it.
If classification was still correct $\epsilon$ value was increased.
\\
The smallest value needed to cause miss classification was saved.
$\overline{\epsilon}$ was compared for different values of pretext training epochs, downstream training epochs,
and kinds of pretext tasks.
\\
In order to compare how pretext tasks preform in comparison with other popular state of the art pre-training technics perform,
I will also compare EffNet trained with pretext tasks by myself with EffNet pre-trained on ImageNet utilized in transfer learning manner.