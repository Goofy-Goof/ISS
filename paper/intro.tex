\section{Introduction}

\paragraph{Motivation}
In recent years, Neural NNs have had a lot of success in image classification.
This has led to advances in many areas, including computer vision and object recognition.
E.g. advances in a constantly ongoing "ImageNet classification challenge", which were achieved
through new approaches,
rather than increasing NNs number of layers and parameters~\cite{russakovsky2015imagenet,DBLP:journals/corr/abs-1905-11946}.
However, NNs are also vulnerable to adversarial attacks~\cite{ilyas2019adversarial}.
Adversarial attacks are inputs designed to fool NN into miss-classifying data.
This can have serious consequences, as it can lead to incorrect results or decisions.
It is researchers' obligation to assure ML classifiers' trust-worthiness.

\paragraph{Adversarial attack}
Goodfellow defined adversarial attacks as “inputs to machine learning models that an
attacker has intentionally designed to cause the model to make a mistake.”
~\cite{DBLP:journals/corr/abs-1802-08195} \\
In the domain of image classification, adversarial attacks are usually formed by applying a small perturbation (which
is barely noticeable for human viewer) to a naturally occurring image, with the intention of making NN miss-classify.
There are many types of adversarial attacks.
This paper focuses on white-box attacks, where the attacker has full access to the model and its parameters,
namely FGSM.

\paragraph{FGSM}
This method was first introduced by Goodfellow and Jonathon Shlens and Christian Szegedy
~\cite{goodfellow2015explaining}.
It produces adversarial images which make NN miss-classify,
but are still recognisable as of the same class for human viewer.
An example of such images can be seen in~\ref{fig:fig-adv}.
FGSM works by using the gradients of the neural network to create an adversarial pattern.
For an input image,
the method evaluates the signed gradient of the loss function with respect to the input image to create a pattern,
which maximizes the loss.
The pattern is then added pixel wise to the original image.
The new image is called the adversarial image.
The process can be summarised using the following expression~\ref{eq:adv}.
\begin{equation}\label{eq:adv}
    adv\_x = x + \epsilon \cdot sign(\nabla_x J(\theta, x, y))
\end{equation}
where: \\
$adv_x$ is the resulting image, \\
$x$ is the original image, \\
$\epsilon$ is the intensity of adversarial pattern, \\
$J$ is the loss function, \\
$\theta$ is NNs parameters, \\
$y$ is the original label.
\\
\begin{figure}[h]
    \begin{subfigure}{0.4\textwidth}
        \caption{Tulips 40\% confidence}
        \includegraphics[width=8cm]{images/og_image}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \caption{Adversarial pattern for this image}
        \includegraphics[width=8cm]{images/adv_pattern}
    \end{subfigure}
    \\
    \begin{subfigure}{0.4\textwidth}
        \caption{$\epsilon = 0.01$, Roses 40\% confidence}
        \includegraphics[width=8cm]{images/adv_attack_001}
    \end{subfigure}
    \begin{subfigure}{0.4\textwidth}
        \caption{$\epsilon = 0.1$, Roses 40\% confidence}
        \includegraphics[width=8cm]{images/adv_attack_01}
    \end{subfigure}

    \caption{An example image, adversarial pattern created from it, and adversarial attack with different values of $\epsilon$}
    \label{fig:fig-adv}
\end{figure}



\paragraph{some meanigfull transition} bla bla bla
\\
\\
One should always want to train ML classifier, in such a way, which maximizes generalization.
Naive approach for this would be to provide as much diverse training data as possible.
This approach, however, heavily relies on a collection and labeling of data by humans, which is an extremely time-consuming

\paragraph{Transfer Learning}
Another popular technic aiming to improve ML classifiers accuracy is called Transfer Learning (TL).
TL is a training approach in ML, which focuses on storing knowledge gained while solving one
problem and applying it to a different but related problem.
For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.
Concrete for ML engineers this could mean reusing one of the NNs, which performed the best at e.g.
"ImageNet classification challenge", and then fine-tuning it for a concrete downstream task of interest.
This eliminated the need of collecting and labeling more data for a concrete classification problem,
however, it still heavily relies on the existence of big labeled datasets.
Not to forget, that while NN reuses the learned features, the NN will also reuse
the biases learned from previous datasets, which makes them once again more dependable on quality of data
collected previously by a human.


\paragraph{Self-Supervised Learning Framework}
In contrast to previously described frameworks, the Self-Supervised Learning framework, requires relatively small amount
of data to produce a big variety of labeled training data.
One of its core features, is that
labeling of new data is not dependent on humans, and can be done in seconds by the ML classifier itself.
E.g. NN applies multiple pre-defined data augmentations to existing data and overwrites the labels by corresponding
pseudo-labels, and then is trained to predict which one was applied.
This allows multiplying the size of
training dataset by a number of pre-defined documentations, and also increasing the diversity of training data
(the metric to measure this, is out of the scope of this paper).

Normally, after self-supervised learning, similarly to TL fine-tuning would be executed.
These parts of training are called pre-text task (or training) and downstream task (or training) respectively.

\paragraph{Pretext task}
Pretext task is the self-supervised learning task solved to learn visual representations,
with the aim of using the learned representations or model weights obtained in the process, for the downstream task.
More concrete for image-classification, the weights of convolutional layers will be frozen after pre-text training,
with the aim of reusing the features learned, similarly to TL.

It has been shown that pretext tasks can significantly improve NNs accuracy~\cite{kolesnikov2019revisiting}.
It is also believed they contribute to NNs learning of important (as per human agents) features.
This paper focuses on TL, rotation and jigsaw pre-text tasks.


\paragraph{Related wörk}
The adversarial vulnerability phenomenon is widely known, first described by Ian J. Goodfellow~\cite{goodfellow2015explaining}.
It has been the accompanying success of image classification over the last years.
Up to day, there is no clear solution to this problem.
One of the suggested approaches is Adversarial Training~\cite{https://doi.org/10.48550/arxiv.1805.12152},
during which NN is trained to reduce the loss over worst-case adversarial perturbations.
This, however, comes at a cost of decreased standard classification accuracy, not the least this approach is noticeably more time-consuming
There have also been numerous studies~\cite{kolesnikov2019revisiting,DBLP:journals/corr/NorooziF16,DBLP:journals/corr/abs-1912-01991}
on how pretext tasks, and their choice can influence standard classification accuracy.
On the other hand, there were no studies, which investigated the impact of pretext task choice on
adversarial vulnerability. \\
This study was motivated by \st{curiosity} increasing demand in highly accurate robust trust-worthy ML classifiers.




